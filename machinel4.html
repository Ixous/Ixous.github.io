
<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Import Katex -->
    <link rel="stylesheet" href="katex/katex.min.css">
    <script src="katex/katex.min.js"></script>
    <!-- Import Katex auto-render -->
    <script src="katex/contrib/auto-render.min.js"></script>

    <meta charset="UTF-8">
    <link rel="stylesheet" href="articles.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
    <title>Reinforcement learning</title>
</head>
<body>
    <div id="content">
        <a id="indexlink" href="index.html" style="margin-bottom: 30px">
            Go back to the list of projects
        </a>
        <img class="banner_img" src="machinel4.png" alt="">
        <h1 id="page_title">
            Reinforcement learning
        </h1>
        <div id="page_date">
            24/11/2019
        </div>

        <p class="series-header">
            This article is part of my series of projects around Machine Learning. <a href="index-machinel.html">Click here to see the list of projects of this series.</a>
        </p>

        <p class="text">
            This project is the fourth and final assignment of CS-7641 Machine Learning at the Georgia Institute of Technology. The assignment is to solve two Markov Decision Processes using three different algorithms.
        </p>


        <h2 class="chapter">Methodology</h2>


        <h3 class="section">Tools</h3>

        <p class="text">
            This project was done in Python using Visual Studio Code and the Jupyter extension. I programmed the problems and algorithms using libraries Numpy, MatPlotLib and IPython.
        </p>

        <h3 class="section">Algorithms</h3>

        <p class="text">
            I designed two problems for this assignment and will explain them in the next part. I will try to solve those problems using the Markov Decision Process algorithms Value Iteration <a class="ref_link" href="#Bellman1957"><sup>[1]</sup></a>, Policy Iteration <a class="ref_link" href="#Howard1960"><sup>[2]</sup></a> and Q-Learning <a class="ref_link" href="#Christopher1992"><sup>[3]</sup></a>.
        </p>

        <h3>Metrics</h3>

        <p class="text">
            To assess the behavior and efficiency of the algorithms, I will measure and report different metrics.
        </p>
        <p class="text">
            When the algorithms try to improve their policy, I calculate a distance between the old and new policy by considering them as vectors consisting of the action to take for each state and using Euclidian distance. This will be plotted in <span style="background-color: steelblue; color: white">blue</span>.
        </p>
        <p class="text">
            After each iteration of the algorithm, the performance of the best policy is measured by running the problem hundreds of times to calculate the average, minimum and maximum scores. This will be plotted in <span style="background-color: indianred; color: white">red</span> with the min-max envelope in a lighter red.
        </p>
        <p class="text">
            For each algorithm and problem, I also plot the computation time per state and per state-action combination. Keep in mind that this measure corresponds to the performance of my computer and could have varied from one run to another due to thermal throttling, other processes, etc. This will be plotted in <span style="background-color: mediumseagreen; color: white">green</span> and <span style="background-color: goldenrod; color: white">yellow</span>.
        </p>


        <h2>Problems</h2>


        <h3>Catch the fruits</h3>

        <p class="text">
            The first problem I implemented is the classic game where the player controls a basket (ü§≤) horizontally at the bottom of the screen and tries to catch falling fruits (üçé) while avoiding falling obstacles (üí©). Catching the former rewards a positive score while the latter reduces the score. The player can move the basket one tile left or right at each turn and is blocked at the two sides.
        </p>
        <div class="center_block">
            <img class="image_tiny dark_invert" src="machinel4/CTF3x3.png" alt="">
            <p class="caption">3√ó3 grid</p>
        </div>
        <p class="text">
            The parameters of the problem are the width and height of the grid (excluding the basket, the width is also the number of positions the basket can be in), the probabilities of each item appearing at the top of the screen at each turn and the rewards for each item.
        </p>
        <div class="center_block">
            <div class="image_small">
                <img class="dark_invert" style="width: 53%;" src="machinel4/CTF4x3.png" alt="">
                <img class="dark_invert" style="width: 40%;" src="machinel4/CTF3x4.png" alt="">
            </div>
            <p class="caption">4√ó3 and 3√ó4 grids</p>
        </div>
        <p class="text">
            This problem can have interesting situations where moving to catch a fruit leads to a position where it is impossible to avoid an obstacle (a long and short grid makes it impossible to predict those traps while a thin and tall grid allows near perfect foresight) :
        </p>
        <div class="center_block">
            <img class="image_tiny dark_invert" src="machinel4/CTFchallenge.png" alt="">
            <p class="caption">
                Staying in this position to catch the fruit
                necessarily leads to catching an obstacle
            </p>
        </div>
        <p class="text">
            The states are represented as an int for the position of the basket and a ternary array for the falling items (0 for nothing, 1 for a fruit and 2 for an obstacle). Therefore, even if the basic 3√ó3 configuration is a small problem with 59k states, the number of states grows exponentially, and faster with the width than the height.
        </p>
        <p class="text">
            The basic 3√ó3 grid has 59k states (177k state-action combinations) ; the 3√ó4 has 1.6M states (4.8M state-action combinations)  ; the 4√ó3 has 2.1M states (8.4M state-action combinations)  ; and the 4√ó4 has 172M states (688M state-action combinations). Therefore, the study can practically only concern the first three configurations.
        </p>
        <p class="text">
            The problem is animated with Python print statements :
        </p>
        <div class="center_block">
            <img class="image_tiny" src="machinel4/CTF 3x4.gif" alt="">
            <p class="caption">One of the resulting AIs playing CTF on a 3√ó4 grid</p>
        </div>

        <h3>Reaching arm</h3>

        <p class="text">
            The second problem I implemented is inspired by OpenAI Gym‚Äôs Reacher v2. It consists of a robotic arm fixed at the center of the field. Each joint of the arm can rotate infinitely. The goal is to touch a target with the effector. At each instant, the robot can move of one angular unit for each joint and gain a small reward based on the distance to the target (calculated with exponential decay). It the robot reaches the target, it is awarded a large score and the target teleports to a new random point.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert_brighter" src="machinel4/reacher2.png" alt="">
            <p class="caption">Reacher with 2 arm pieces</p>
        </div>
        <p class="text">
            The parameters of this problem are the number of arm pieces, the angular resolution for the joints, the target resolution, and the rewards coefficients. A high enough angular resolution is necessary to have a correct simulation (around 100 steps per revolution). The targets spawn on a grid of the target resolution squared, this value can be much smaller (around 10). Adding an arm piece increases the number of states tremendously as it is multiplied by the angular resolution.
        </p>
        <p class="text">
            Therefore, I will study three configurations : 2 arm pieces of resolution 60 with target resolution of 6 (129k states, 1.2M state-action combinations) ; 2 arm pieces of resolution 100 with target resolution of 10 (1M states, 9M state-action combinations) ; and 3 arm pieces of resolution 50 with target resolution of 6 (4.5M states, 121M state-action combinations).
        </p>
        <div class="center_block">
            <img class="image_two dark_invert_brighter" src="machinel4/reacher3.png" alt="">
            <p class="caption">Reacher with 3 arm pieces</p>
        </div>
        <p class="text">
            The problem is visualized using Matplotlib‚Äôs animations :
        </p>
        <div class="center_block">
            <img class="image_two dark_invert_brighter" src="machinel4/R2.gif" alt="">
            <img class="image_two dark_invert_brighter" src="machinel4/R3.gif" alt="">
            <p class="caption">This is an AI controlling the arm</p>
        </div>


        <h2>Value Iteration</h2>


        <h3>Catch the fruits</h3>

        <p class="text">
            I first ran the algorithm on the small simplified problem with a 3√ó3 grid with 1.5k states with a discount factor Œ≥ of 0.5, 0.9 and 0.99.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image4.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image6.svg" alt="">
        </div>
        <p class="text">
            As expected, for such a small problem, value iteration was able to find an optimal policy in a single iteration and the discount value has no noticeable impact on either performance improvement or policy convergence. This can be explained as in this problem, the agent only need to care about the next few rows of object and not the distant future (it actually only needs as many rows as the width of the grid which number of possible positions of the hand and the number of moves required to go from one side to the other plus one, this will be explained in more detailed later).
        </p>
        <p class="text">
            I then ran the algorithm on the more complex problem 3√ó3 problem with 59k states with a discount factor Œ≥ of 0.5, 0.9 and 0.99.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image8.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image10.svg" alt="">
        </div>
        <p class="text">
            As we can see, for all Œ≥, the maximum average performance was reached after 2 iterations only, and even just one iteration gets us to near best performance.
        </p>
        <p class="text">
            However, the larger the Œ≥, the slower the policy converges. The early stop for a Œ≥ of 0.5 is due to the fact that the values U converges much faster (here it is stopped when the maximum U difference between two iterations for each state is less than 0.001).
        </p>
        <p class="text">
            Therefore, for this dimension of this problem, a rough policy is sufficient to reach best performance and the discount coefficient Œ≥ can be lowered to speed up convergence after that. The initial policy at iteration 0 is the policy of random actions. Therefore; after two iterations, we can read on the graph that only about 380 policy changes (changes as in distance in Euclidian metrics) were required to reach optimal performance, and the other changes until convergence didn‚Äôt affect the performance visibly.
        </p>
        <p class="text">
            Finally, I tried running the algorithm on the 3√ó4 problem with 1.6M states with a discount factor Œ≥ of 0.1, 0.5 and 0.9. Unfortunately, I lost the policy change graph.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image12.svg" alt="">
        </div>
        <p class="text">
            The result is not very interesting, with the same behavior as for the 3√ó3 size and with the same score (the agent can at best pick up one fruit per turn). A taller grid gives the agent more knowledge of what falling items will arrive but with a width of 3, the furthest the agent can move is 2, thus needs a height of only 3 for perfect planning, anything above that is useless to him, as we can observe.
        </p>
        <p class="text">
            The higher number of states made the solving of a larger configuration practically impossible. Aside from the RAM usage, on my computer, value iteration takes about 45 seconds per iteration for the 3x3, 35 minutes for the 3x4 and 45 hours for the 4x3 that I had to stop after 8 hours 20 minutes when it had covered only 390k states of the 2.1M. And for the smaller simplified problem with the fruits only, each iteration took under a second.
        </p>

        <h3>Reaching arm</h3>

        <p class="text">
            I first ran the algorithm on the problem with 2 arm pieces with angular resolution of 60 and target resolution of 6 with 129k states with a discount factor Œ≥ of 0.5, 0.9 and 0.99.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image14.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image16.svg" alt="">
        </div>
        <p class="text">
            Here, a smaller discount factor makes the policy converge faster and reach the maximal performance earlier as well.
        </p>
        <p class="text">
            Again, we can conclude that this problem with the rewards the way they are doesn‚Äôt need to consider the rewards of faraway states to converge to an optimal policy and considering too much the far future pollutes the values and slows down convergence. I hypothesized that by removing the gradient distance rewards, the policy will converge far slower if at all.
        </p>
        <p class="text">
            To test this, I ran the same test but with the gradient reward disabled.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image18.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image20.svg" alt="">
        </div>
        <p class="text">
            However, we observe that only the run with the lowest value of Œ≥ was affected. Therefore, the large rewards of reaching the targets propagates fast enough for high discount factors, but for a small discount factor, gradient rewards help it converge faster.
        </p>
        <p class="text">
            I then increased the dimensions of the problem by increasing the angular resolution of the joints from 60 to 100 and the target resolution from 6 to 10.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image22.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image24.svg" alt="">
        </div>
        <p class="text">
            We can once again observe that smaller discount value makes the algorithm converge much faster. However, it seems that a value of Œ≥ of 0.1 and 0.5 leads to average performance slightly smaller than for 0.9.
        </p>
        <p class="text">
            I then increased the dimensions of the problem even further by adding a third arm piece but reducing the angular resolution to 50.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image26.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image28.svg" alt="">
        </div>
        <p class="text">
            While the smaller discount value initially leads to faster performance growth, both converge to the optimal performance around the same time after 17 iterations but the smaller value stalled for a few iterations on a suboptimal policy while it made changes until it was able find a breakthrough. A stopping iteration condition based on performance alone would have prevented this breakthrough, one based on policy changes or value changes is therefore preferable.
        </p>
        <p class="text">
            Again, a lower discount factor makes the policy converge faster and a higher value keeps improving the policy for more iteration with no noticeable impact of the performance.
        </p>

        <h3>Computation time</h3>

        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image30.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image32.svg" alt="">
        </div>
        <p class="text">
            The relation between computation time and either number of states or state-action combinations seems very roughly linear with both linear axes, hence linear.
        </p>
        <p class="text">
            Moreover, we can still observe that increasing even one dimension of a problem increases the number of states and state-action combination enormously and thus the computation time as well.
        </p>


        <h2>Policy Iteration</h2>


        <p class="text">
            For each problem, the policy iteration was limited to 20 global iterations and 10 iterations of evaluation per global iteration.
        </p>

        <h3>Catch the fruits</h3>

        <p class="text">
            I first ran the algorithm on the small simplified problem with a 3√ó3 grid with 1.5k states with a discount factor Œ≥ of 0.5, 0.9 and 0.99.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image34.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image36.svg" alt="">
        </div>
        <p class="text">
            As with value iteration, the problem is just so simple that the algorithm is able to find an optimal policy very fast. A smaller value of the discount factor seems to lead to the optimal policy in a single iteration, while a larger value like 0.99 requires two iterations.
        </p>
        <p class="text">
            This is due to the fact that giving more weight to the far future is useless to the decision of the agent as explained earlier, and this pollutes the policy improvement and thus slows down the convergence slightly.
        </p>
        <p class="text">
            I then ran the algorithm on the more complex problem 3√ó3 problem with 59k states with a discount factor Œ≥ of 0.1, 0.5 and 0.9.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image38.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image40.svg" alt="">
        </div>
        <p class="text">
            Here a very small value of the discount factor slows down the improvement of performance. Also, once again, a smaller value of Œ≥ also makes the policy converge faster and a larger value makes the algorithm change the policy without noticeable effect on performance.
        </p>
        <p class="text">
            The computation time was about 6 minutes per iteration for the 3x3, and so long for larger grids that I wasn‚Äôt able to get even one iteration to know. And for the smaller simplified problem with the fruits only, each iteration took about a second.
        </p>

        <h3>Reaching arm</h3>

        <p class="text">
            I first ran the algorithm on the problem with 2 arm pieces with angular resolution of 60 and target resolution of 6 with 129k states. As this algorithm is much slower, I had to limit my study to a discount factor of 0.5 only.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image42.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image44.svg" alt="">
        </div>
        <p class="text">
            Policy iteration reaches the same maximal score as value iteration, which we can assume (and confirm by looking at the animation of the agent playing) that it is what we would expect as the best possible policy.
        </p>
        <p class="text">
            Convergence to the maximal score is as fast as for value iteration but policy iteration stops improving the policy after about 14 iterations while value iteration kept making changes.
        </p>
        <p class="text">
            The algorithm is so slow that I wasn‚Äôt able to complete the training of an agent for the larger versions of this problem or for multiple values of the discount factor. I was just able to estimate the time each iteration could have taken.
        </p>

        <h3>Computation time</h3>

        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image46.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image48.svg" alt="">
        </div>
        <p class="text">
            We observe similar behavior to value iteration but this algorithm is slower by about a factor of 10 for Catch The Fruit and 10000 for the Reacher.
        </p>


        <h2>Q Learning</h2>


        <p class="text">
            As the number of hyperparameters of this algorithm is greater than the others, I will study them on the smallest problems then extrapolate for the larger.
        </p>
        <p class="text">
            Q-learning is a MDP algorithm that is blind to the formula of rewards and transitions. Hence, it has to explore the state and state-action spaces. There exist several exploration strategies, I will study some of them.
        </p>

        <h3>Catch the fruits</h3>

        <p class="text">
            I started by studying this algorithm on the small simplified version of the problem. However, with the basic exploration strategy of choosing the action leading to the best Q value, the algorithm never improved.
        </p>
        <p class="text">
            Thus, I tried a common exploration strategy : Œµ-greedy exploration, which consist in choosing the Q-best action most but not all of the time, and choosing a random action with a probability of Œµ. This allows the algorithm to explore more of the state-action space before converging. For this study, I arbitrarily chose a discount factor of 0.5 and a learning rate of 0.5. (Here one tick on the plot is 100 iterations).
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image50.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image52.svg" alt="">
        </div>
        <p class="text">
            As we can see, a value of Œµ of 0, corresponding to the default exploration strategy described above never lets the algorithm explore enough to learn and performs no better than the random policy where Œµ equals 1.
        </p>
        <p class="text">
            Then, above from 0, the performance increases with lower values of Œµ. Thus, we have to study this parameter more precisely. We can also note that the policy always changes and more so with higher values of Œµ, but a maximum performance is reaches for all values after less than 500 iterations. Hence we can study the average performance after 1000 iterations for different values of Œµ :
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image54.svg" alt="">
        </div>
        <p class="text">
            As we can see, an ideal value of Œµ is contained within the 0.05 to 0.2 range (here the optimal score was reached for all values in this range). A value of 0.2 will be used in the following experiments.
        </p>
        <p class="text">
            Note that the best performances reached match that of the other two algorithms that, unlike Q-learning, have perfect knowledge of the rewards and transitions.
        </p>
        <p class="text">
            Then, using this exploration strategy, I studied the effect of the learning rate Œ±.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image56.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image58.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image60.svg" alt="">
        </div>
        <p class="text">
            As we can see, the learning rate has no observable effect on the performance after convergence or the speed of convergence either. An arbitrary value of 0.5 will be used for following experiments.
        </p>
        <p class="text">
            I then studied the effect of the discount factor Œ≥ similarly to the experiments for the other algorithms. (Again, one tick on the plot is 100 iterations).
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image62.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image64.svg" alt="">
        </div>
        <p class="text">
            A smaller value of gamma seems to lead to a faster convergence and better performance. The policy always changes, as seems to always be the case with Q-learning and Œµ-greedy exploration, which makes sense as the random exploration would require a very large amount of iterations to converge.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image66.svg" alt="">
        </div>
        <p class="text">
            As we can see, a very high discount factor leads to poor performance but anything below 0.9 is good. The explanation for this is the same as the one given for the previous algorithms.
        </p>
        <p class="text">
            I also studied other exploration strategies : Optimistic Initialization encourages the algorithm to explore new state-action combinations by initializing all Q values to a non-zero value (here 1) ; what I will call Œµp-Random (for epsilon-probabilistically random) choses the action randomly proportionally to their Q value but with a basic probability of Œµ divided by the number of possible actions (here Œµ is 0.1 meaning for 3 actions, each has a basic probability of 0.033, then the remaining 0.9 is shared proportionally to the Q values) ; what I will call ŒµŒ≤p-Random is similar to Œµp-Random but instead of using proportionality to the Q values, it is to the QŒ≤ values thus allowing to tune how much it will commit to better values of Q or pick less a-priori preferable actions, here I took a Œ≤ of 3 and Œµ of 0.1 (note that Œµp-Random is a special case of ŒµŒ≤p-Random with a Œ≤ of 1). I used the previously determined values : Œµ of 0.2 for Œµ-greedy exploration, Œ± of 0.5 and Œ≥ of 0.5 for this study.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image68.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image70.svg" alt="">
        </div>
        <p class="text">
            As I increased the number of runs the average performance is calculated with, I conclude that those variations stem from the algorithm itself.
        </p>
        <p class="text">
            Optimistic initialization performs the best followed by ŒµŒ≤p-Random and closely after Œµ-Greedy while Œµp-Random performs significantly worse. A more complete study of the hyperparameters of those exploration strategies could lead more reliable results though. Œµp-Random and ŒµŒ≤p-Random are however significantly slower than the two others.
        </p>
        <p class="text">
            A more extensive study with more time would experiment with the hyperparameters of these exploration strategy and include other strategies such as UCB1 and Boltzmann exploration.
        </p>
        <p class="text">
            I then applied those exploration strategies to the more complex 3√ó3 problem with the obstacles.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image72.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image74.svg" alt="">
        </div>
        <p class="text">
            Here, optimistic initialization actually performed better than Œµ-greedy and converged with fewer changes to the policy, but neither have reached a plateau in the iterations of this experiment. Œµp-Random is barely better than the random policy in the amount of iterations studied here but still makes many changes to the policy so a longer training time could lead to different results. ŒµŒ≤p-Random is slightly better than Œµp-Random but it also needs more iterations, though it looks like it could converge faster than Œµp-Random.
        </p>
        <p class="text">
            The required computation time were way too long to practically study the strategies or the parameters on the larger versions of the problem.
        </p>

        <h3>Reaching arm</h3>

        <p class="text">
            I only ran the experiment for Œµ-greedy and optimistic initialization with the same values for the parameters as determined for the other problem as the computation times were much greater and it was practically impossible to study all the parameters and all the strategies. I also only ran the full experiment on the problem with 2 arm pieces, angular resolution of 60 and target resolution of 6.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image76.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image78.svg" alt="">
        </div>
        <p class="text">
            Unlike the previous problem, here Œµ-greedy performs better than optimistic initialization. However, neither come even close to the performance seen with the other two algorithms that, unlike Q-learning, have perfect knowledge of the rewards and transitions. Note that this experiment took too long and the policy didn‚Äôt converge so the optimal performance could be reached but with a very long computation time, far longer than the other two algorithms.
        </p>

        <h3>Computation time</h3>

        <p class="text">
            All computation times of this section are calculated for 1000 Q-learning iterations with 200 turns for each starting state and using TQDM‚Äôs measurement and estimations.
        </p>
        <p class="text">
            First, I measured the execution time for the different exploration strategies :
        </p>
        <table>
            <tr>
                <td>Œµ-greedy</td>
                <td>25 sec</td>
            </tr>
            <tr>
                <td>optimistic</td>
                <td>25 sec</td>
            </tr>
            <tr>
                <td>Œµp-random</td>
                <td>30 sec</td>
            </tr>
            <tr>
                <td>ŒµŒ≤p-random</td>
                <td>35 sec</td>
            </tr>
        </table>
        <p class="text">
            Keep in mind that these values are rough estimates, but they are coherent with what we can expect looking at the computation required for each.
        </p>
        <p class="text">
            Then for each problem and some problem sizes with Œµ-greedy exploration :
        </p>
        <table>
            <tr>
                <th>Problem</th>
                <th>Number of states</th>
                <th>Number of S-A</th>
                <th>Computation time</th>
            </tr>
            <tr>
                <td>CTFS 3√ó3</td>
                <td>1.5k</td>
                <td>4.5k</td>
                <td>25 sec</td>
            </tr>
            <tr>
                <td>CTF 3√ó3</td>
                <td>59k</td>
                <td>177k</td>
                <td>120 sec</td>
            </tr>
            <tr>
                <td>CTF 3√ó4</td>
                <td>1.6M</td>
                <td>3.8M</td>
                <td>2400 sec</td>
            </tr>
            <tr>
                <td>CTF 4√ó3</td>
                <td>2.1M</td>
                <td>8.4M</td>
                <td>2700 sec</td>
            </tr>
            <tr>
                <td>CTF 4√ó4</td>
                <td>172M</td>
                <td>688M</td>
                <td>memory error</td>
            </tr>
            <tr>
                <td>R2 60 6</td>
                <td>128k</td>
                <td>1.2M</td>
                <td>900 sec</td>
            </tr>
            <tr>
                <td>R2 100 10</td>
                <td>1M</td>
                <td>9M</td>
                <td>6600 sec</td>
            </tr>
            <tr>
                <td>R3 50 6</td>
                <td>4.5M</td>
                <td>121M</td>
                <td>memory error</td>
            </tr>
        </table>
        <div class="center_block">
            <p class="caption">
                CTF = catch the fruit ; CTFS = catch the fruit simplified (no obstacle) ; R2/R3 = reaching arm with 2 or 3 pieces ; first parameter of R is the angular resolution of each arm piece ; second parameter of R is the number of possible coordinates of the targets along each axis
            </p>
        </div>
        <p class="text">
            Problems with more than a few million states and/or tens of millions of state-action combinations requires too much memory with Q-learning than my computer can provide (Python used almost 12Gb before crashing in those cases).
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image80.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image82.svg" alt="">
        </div>
        <p class="text">
            Keep in mind that this is for 1000 iterations, which was enough for the Catch The Fruit Simplified problem to converge, but Catch The Fruit 3√ó3 would have required about 3000 and Reacher 2-60-6 over 10000. While the other two algorithms converged in 1 to 5 iterations.
        </p>
        <div class="center_block">
            <img class="image_two dark_invert" src="machinel4/media/image84.svg" alt="">
            <img class="image_two dark_invert" src="machinel4/media/image86.svg" alt="">
        </div>
        <p class="text">
            Nevertheless, 1000 iterations of Q-learning take longer than one iteration of Value Iteration or Policy Iteration for the smallest versions of the problems, while it takes significantly less time for the larger versions of the problems.
        </p>


        <h2>Conclusions</h2>


        <p class="text">
            All-in-all, when we have access to the reward and transition functions, both Policy Iteration and Value Iteration perform perfectly on small problems (hundreds to low thousands of states). Policy Iteration seems to converge in fewer iterations than Value Iteration for large problems (hundreds of thousands to low millions of states) but takes longer to compute.
        </p>
        <p class="text">
            Q-learning doesn‚Äôt need the reward and transition functions and could converge faster than Policy Iteration and Value Iteration for large problems, but is far slower for small problems and requires too much memory for problems with more than a few million states and/or tens of millions of state-action combinations.
        </p>
        <p class="text">
            Moreover, many parameters of the algorithms can and have to be tuned for each algorithm so any conclusions made based on the limited experiments above needs to be conservative.
        </p>


        <h2>References</h2>


        <p class="text"><a name="Bellman1957">[1]</a> Bellman, R. (1957). ‚ÄúA Markovian Decision Process‚Äù. Journal of Mathematics and Mechanics.</p>
        <p class="text"><a name="Howard1960">[2]</a> Howard, Ronald A. (1960). ‚ÄúDynamic Programming and Markov Processes‚Äù. The M.I.T. Press.</p>
        <p class="text"><a name="Christopher1992">[3]</a> Christopher J. C. H. Watkins, Peter Dayan (1992). ‚ÄúQ-learning‚Äù. Machine Learning.</p>


        <br><br>
        <div class="iconlabel">
            <img class="dark_invert" src="github.svg" alt="">
            <a class="link" href="https://github.com/Louis-DR/Reinforcement-Learning">
                See the project on GitHub
            </a>
        </div>

        <p class="series-header">
            This article is part of my series of projects around Machine Learning. <a href="index-machinel.html">Click here to see the list of projects of this series.</a>
        </p>

        <a id="indexlink" href="index.html" style="margin-top: 60px">
            Go back to the list of projects
        </a>
        <script>
            renderMathInElement(document.body,{delimiters: [
                {left: "$$", right: "$$", display: false},
                {left: "¬£¬£", right: "¬£¬£", display: true}
            ]});
        </script>
    </div>
</body>
</html>